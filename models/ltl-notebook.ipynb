{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9a608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install dependencies\n",
    "!pip install \"transformers==4.34.0\" \"datasets[s3]==2.13.0\" \"sagemaker>=2.190.0\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1e2634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup SageMaker session\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8421c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load and explore dataset\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "dataset = load_dataset('json', data_files='s3://uav-ltl-data/lifed_data.jsonl', split='train')\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Sample: {dataset[0]}\")\n",
    "print(f\"Features: {dataset.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70da25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Format and split dataset\n",
    "def format_ltl_sample(sample):\n",
    "    sentence = ' '.join(sample['logic_sentence'])\n",
    "    ltl = ' '.join(sample['logic_ltl'])\n",
    "    \n",
    "    instruction = \"Translate the following natural language description to Linear Temporal Logic (LTL):\"\n",
    "    \n",
    "    text = f\"\"\"### Instruction\n",
    "{instruction}\n",
    "\n",
    "### Input\n",
    "{sentence}\n",
    "\n",
    "### Output\n",
    "{ltl}</s>\"\"\"\n",
    "    \n",
    "    return {\"text\": text}\n",
    "\n",
    "dataset = dataset.map(format_ltl_sample, remove_columns=dataset.column_names)\n",
    "\n",
    "# 80/10/10 split\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "test_valid = dataset['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "eval_dataset = test_valid['train']  \n",
    "test_dataset = test_valid['test']  \n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Eval: {len(eval_dataset)}, Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b820d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Tokenize all splits\n",
    "from transformers import AutoTokenizer\n",
    "import sys\n",
    "sys.path.append(\"../scripts/utils\")\n",
    "from pack_dataset import pack_dataset\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "def tokenize_and_pack(dataset_split):\n",
    "    tokenized = dataset_split.map(\n",
    "        lambda sample: tokenizer(sample[\"text\"]), \n",
    "        batched=True, \n",
    "        remove_columns=[\"text\"]\n",
    "    )\n",
    "    return pack_dataset(tokenized, chunk_length=2048)\n",
    "\n",
    "train_dataset = tokenize_and_pack(train_dataset)\n",
    "eval_dataset = tokenize_and_pack(eval_dataset)\n",
    "test_dataset = tokenize_and_pack(test_dataset)\n",
    "\n",
    "print(f\"Packed - Train: {len(train_dataset)}, Eval: {len(eval_dataset)}, Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb31fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Upload all splits to S3\n",
    "train_path = f's3://{sess.default_bucket()}/processed/mistral/ltl-translation/train'\n",
    "eval_path = f's3://{sess.default_bucket()}/processed/mistral/ltl-translation/eval'\n",
    "test_path = f's3://{sess.default_bucket()}/processed/mistral/ltl-translation/test'\n",
    "\n",
    "train_dataset.save_to_disk(train_path)\n",
    "eval_dataset.save_to_disk(eval_path)\n",
    "test_dataset.save_to_disk(test_path)\n",
    "\n",
    "print(f\"Train: {train_path}\")\n",
    "print(f\"Eval: {eval_path}\")\n",
    "print(f\"Test: {test_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Configure training with early stopping\n",
    "hyperparameters = {\n",
    "    # Model & Data\n",
    "    'model_id': model_id,\n",
    "    'dataset_path': '/opt/ml/input/data/training',\n",
    "    'output_dir': '/tmp/run',\n",
    "    'merge_adapters': True,\n",
    "    \n",
    "    # Core Training\n",
    "    'num_train_epochs': 5,\n",
    "    'per_device_train_batch_size': 6,\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    \n",
    "    # Learning Rate\n",
    "    'learning_rate': 2e-4,\n",
    "    'lr_scheduler_type': 'cosine',\n",
    "    'warmup_ratio': 0.03,\n",
    "    'max_grad_norm': 0.3,\n",
    "    \n",
    "    # Memory/Speed\n",
    "    'gradient_checkpointing': True,\n",
    "    'bf16': True,\n",
    "    'tf32': True,\n",
    "    'use_flash_attn': True,\n",
    "    \n",
    "    # Logging/Saving\n",
    "    'logging_steps': 10,\n",
    "    'save_strategy': 'steps',\n",
    "    'save_steps': 200,\n",
    "    \n",
    "    # Evaluation & Early Stopping\n",
    "    'evaluation_strategy': 'steps',\n",
    "    'eval_steps': 200,\n",
    "    'load_best_model_at_end': True,\n",
    "    'metric_for_best_model': 'eval_loss',\n",
    "}\n",
    "\n",
    "job_name = 'mistral-ltl-translation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708b8ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Create training estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='run_qlora.py',\n",
    "    source_dir='../scripts',\n",
    "    instance_type='ml.g5.4xlarge',\n",
    "    instance_count=1,\n",
    "    max_run=2*24*60*60,\n",
    "    base_job_name=job_name,\n",
    "    role=role,\n",
    "    volume_size=300,\n",
    "    transformers_version='4.37',\n",
    "    pytorch_version='2.0',\n",
    "    py_version='py310',\n",
    "    hyperparameters=hyperparameters,\n",
    "    environment={\"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\"},\n",
    "    disable_output_compression=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038345b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Start training with eval data\n",
    "data = {\n",
    "    'training': train_path,\n",
    "    'evaluation': eval_path\n",
    "}\n",
    "\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2832d584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Deploy model (optional - after training completes)\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "llm_image = get_huggingface_llm_image_uri(\"huggingface\", version=\"1.1.0\", session=sess)\n",
    "\n",
    "model_s3_path = huggingface_estimator.model_data[\"S3DataSource\"][\"S3Uri\"]\n",
    "\n",
    "config = {\n",
    "    'HF_MODEL_ID': \"/opt/ml/model\",\n",
    "    'SM_NUM_GPUS': '1',\n",
    "    'MAX_INPUT_LENGTH': '1024',\n",
    "    'MAX_TOTAL_TOKENS': '2048',\n",
    "}\n",
    "\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "llm_model = HuggingFaceModel(\n",
    "    role=role,\n",
    "    image_uri=llm_image,\n",
    "    model_data={'S3DataSource': {'S3Uri': model_s3_path, 'S3DataType': 'S3Prefix', 'CompressionType': 'None'}},\n",
    "    env=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5492ebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Test inference\n",
    "llm = llm_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.g5.2xlarge',\n",
    "    container_startup_health_check_timeout=300\n",
    ")\n",
    "\n",
    "# Test translation\n",
    "test_input = \"Globally, everytime when prop_2 and prop_1 then prop_3\"\n",
    "prompt = f\"\"\"### Instruction\n",
    "Translate the following natural language description to Linear Temporal Logic (LTL):\n",
    "\n",
    "### Input\n",
    "{test_input}\n",
    "\n",
    "### Output\n",
    "\"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9\n",
    "    }\n",
    "}\n",
    "\n",
    "response = llm.predict(payload)\n",
    "print(response[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
